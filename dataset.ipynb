{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7840afca",
   "metadata": {},
   "source": [
    "# Electricity demand forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4378dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from meteostat import Hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37d23ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_power_plant_data(path: str = 'data/power_plant.csv') -> pd.DataFrame:\n",
    "    df_power_plant = pd.read_csv(path, delimiter=';')\n",
    "    df_power_plant = df_power_plant[df_power_plant['Power'] != '-']\n",
    "    df_power_plant['Power'] = df_power_plant['Power'].str.replace(',', '.').astype(float)\n",
    "    df_power_plant['Datetime'] = pd.to_datetime(df_power_plant['Datetime'], format='%d.%m.%Y %H:%M')\n",
    "    df_power_plant = df_power_plant.reindex(columns=['Datetime', 'Power'])\n",
    "    return df_power_plant\n",
    "\n",
    "df = load_power_plant_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "761c86b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_holiday_data(years: list[int], region: str = 'de-be') -> pd.DataFrame:\n",
    "    holiday_dates = []\n",
    "    for year in years:\n",
    "        url = f\"https://digidates.de/api/v1/germanpublicholidays?year={year}&region={region}\"\n",
    "        response = requests.get(url)\n",
    "        holidays = response.json()\n",
    "        [holiday_dates.append(holiday) for holiday in holidays.keys()]\n",
    "\n",
    "    df_holidays = pd.DataFrame(data={\"Holiday\": holiday_dates})\n",
    "    return df_holidays\n",
    "\n",
    "# years = df['Datetime'].dt.strftime(\"%Y\").unique()\n",
    "# df_holidays = fetch_holiday_data(years=years)\n",
    "# df_holidays.to_csv('data/holidays.csv')\n",
    "\n",
    "df_holidays = pd.read_csv('data/holidays.csv')\n",
    "df_holidays['Holiday'] = pd.to_datetime(df_holidays['Holiday'], format='%Y-%m-%d').dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "214af83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_weather_data(start: pd.Timestamp, end: pd.Timestamp, station_id: str) -> pd.DataFrame:\n",
    "    weather = Hourly(station_id, start, end)\n",
    "    df_weather = weather.fetch()\n",
    "    df_weather = df_weather.reset_index()\n",
    "\n",
    "    df_weather = df_weather.rename(columns={\n",
    "        'time': 'Datetime',\n",
    "        'temp': 'Temperature',\n",
    "        'dwpt': 'Dew Point',\n",
    "        'rhum': 'Relative Humidity',\n",
    "        'prcp': 'Precipitation',\n",
    "        'snow': 'Snow Depth',\n",
    "        'wdir': 'Wind Direction',\n",
    "        'wspd': 'Average Wind Speed',\n",
    "        'wpgt': 'Peak Wind Speed',\n",
    "        'pres': 'Average Sea-Level Air Pressure',\n",
    "        'tsun': 'Sunshine Duration',\n",
    "        'coco': 'Weather Condition Code'\n",
    "    })\n",
    "\n",
    "    df_weather['Datetime'] = pd.to_datetime(df_weather['Datetime'])\n",
    "    df_weather = df_weather.set_index('Datetime').sort_index()\n",
    "    df_weather = df_weather.resample('15min').interpolate(method='linear')\n",
    "    df_weather = df_weather.reset_index()\n",
    "    return df_weather\n",
    "\n",
    "# start = df['Datetime'].min()\n",
    "# end = df['Datetime'].max()\n",
    "# df_weather = fetch_weather_data(start=start, end=end, station_id='10582')\n",
    "# df_weather.to_csv('data/weather.csv', index=False)\n",
    "\n",
    "df_weather = pd.read_csv('data/weather.csv')\n",
    "df_weather['Datetime'] = pd.to_datetime(df_weather['Datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9071b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time-based features\n",
    "df['Holiday'] = df['Datetime'].dt.date.isin(df_holidays['Holiday'])\n",
    "df['Hour'] = df['Datetime'].dt.hour\n",
    "df['DayOfWeek'] = df['Datetime'].dt.dayofweek\n",
    "df['Month'] = df['Datetime'].dt.month\n",
    "df['IsWeekend'] = df['DayOfWeek'].isin([5,6]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9947d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset\n",
    "df = pd.merge(df, df_weather, on=['Datetime'], how='left')\n",
    "# df.to_csv('data/dataset.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9003e15a",
   "metadata": {},
   "source": [
    "### Day-ahead market price dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b14ac88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_market_data(path: str = 'data/day_ahead_prices.csv') -> pd.DataFrame:\n",
    "    df_market = pd.read_csv(path, delimiter=';')\n",
    "    df_market['Datetime'] = pd.to_datetime(df_market['Datetime'], format='%Y-%m-%d %H:%M:00')\n",
    "    return df_market\n",
    "\n",
    "df_market = load_market_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a9cb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta\n",
      "0 days 00:15:00    350253\n",
      "0 days 00:00:00        40\n",
      "0 days 01:15:00        10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_market = df_market.sort_values('Datetime')\n",
    "df_market['delta'] = df_market['Datetime'].diff()\n",
    "print(df_market['delta'].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75cb7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lücke zwischen 2015-03-29 01:45:00 und 2015-03-29 03:00:00 — Dauer: 0 days 01:15:00\n",
      "Lücke zwischen 2016-03-27 01:45:00 und 2016-03-27 03:00:00 — Dauer: 0 days 01:15:00\n",
      "Lücke zwischen 2017-03-26 01:45:00 und 2017-03-26 03:00:00 — Dauer: 0 days 01:15:00\n",
      "Lücke zwischen 2018-03-25 01:45:00 und 2018-03-25 03:00:00 — Dauer: 0 days 01:15:00\n",
      "Lücke zwischen 2019-03-31 01:45:00 und 2019-03-31 03:00:00 — Dauer: 0 days 01:15:00\n",
      "Lücke zwischen 2020-03-29 01:45:00 und 2020-03-29 03:00:00 — Dauer: 0 days 01:15:00\n",
      "Lücke zwischen 2021-03-28 01:45:00 und 2021-03-28 03:00:00 — Dauer: 0 days 01:15:00\n",
      "Lücke zwischen 2022-03-27 01:45:00 und 2022-03-27 03:00:00 — Dauer: 0 days 01:15:00\n",
      "Lücke zwischen 2023-03-26 01:45:00 und 2023-03-26 03:00:00 — Dauer: 0 days 01:15:00\n",
      "Lücke zwischen 2024-03-31 01:45:00 und 2024-03-31 03:00:00 — Dauer: 0 days 01:15:00\n"
     ]
    }
   ],
   "source": [
    "gaps = df_market[df_market['delta'] > pd.Timedelta('15min')]\n",
    "for i, row in gaps.iterrows():\n",
    "    start = df_market.loc[i - 1, 'Datetime']\n",
    "    end = row['Datetime']\n",
    "    print(f\"Lücke zwischen {start} und {end} — Dauer: {row['delta']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17539e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lücke zwischen 2015-10-25 02:45:00 und 2015-10-25 02:00:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2015-10-25 02:00:00 und 2015-10-25 02:15:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2015-10-25 02:15:00 und 2015-10-25 02:30:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2015-10-25 02:30:00 und 2015-10-25 02:45:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2016-10-30 02:45:00 und 2016-10-30 02:00:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2016-10-30 02:00:00 und 2016-10-30 02:15:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2016-10-30 02:15:00 und 2016-10-30 02:30:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2016-10-30 02:30:00 und 2016-10-30 02:45:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2017-10-29 02:45:00 und 2017-10-29 02:00:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2017-10-29 02:00:00 und 2017-10-29 02:15:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2017-10-29 02:15:00 und 2017-10-29 02:30:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2017-10-29 02:30:00 und 2017-10-29 02:45:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2018-10-28 02:45:00 und 2018-10-28 02:00:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2018-10-28 02:00:00 und 2018-10-28 02:15:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2018-10-28 02:15:00 und 2018-10-28 02:30:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2018-10-28 02:30:00 und 2018-10-28 02:45:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2019-10-27 02:45:00 und 2019-10-27 02:00:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2019-10-27 02:00:00 und 2019-10-27 02:15:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2019-10-27 02:15:00 und 2019-10-27 02:30:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2019-10-27 02:30:00 und 2019-10-27 02:45:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2020-10-25 02:45:00 und 2020-10-25 02:00:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2020-10-25 02:00:00 und 2020-10-25 02:15:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2020-10-25 02:15:00 und 2020-10-25 02:30:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2020-10-25 02:30:00 und 2020-10-25 02:45:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2021-10-31 02:45:00 und 2021-10-31 02:00:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2021-10-31 02:00:00 und 2021-10-31 02:15:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2021-10-31 02:15:00 und 2021-10-31 02:30:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2021-10-31 02:30:00 und 2021-10-31 02:45:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2022-10-30 02:45:00 und 2022-10-30 02:00:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2022-10-30 02:00:00 und 2022-10-30 02:15:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2022-10-30 02:15:00 und 2022-10-30 02:30:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2022-10-30 02:30:00 und 2022-10-30 02:45:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2023-10-29 02:45:00 und 2023-10-29 02:00:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2023-10-29 02:00:00 und 2023-10-29 02:15:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2023-10-29 02:15:00 und 2023-10-29 02:30:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2023-10-29 02:30:00 und 2023-10-29 02:45:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2024-10-27 02:45:00 und 2024-10-27 02:00:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2024-10-27 02:00:00 und 2024-10-27 02:15:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2024-10-27 02:15:00 und 2024-10-27 02:30:00 — Dauer: 0 days 00:00:00\n",
      "Lücke zwischen 2024-10-27 02:30:00 und 2024-10-27 02:45:00 — Dauer: 0 days 00:00:00\n"
     ]
    }
   ],
   "source": [
    "gaps = df_market[df_market['delta'] == pd.Timedelta('0min')]\n",
    "for i, row in gaps.iterrows():\n",
    "    start = df_market.loc[i - 1, 'Datetime']\n",
    "    end = row['Datetime']\n",
    "    print(f\"Lücke zwischen {start} und {end} — Dauer: {row['delta']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffb02ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pja",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
